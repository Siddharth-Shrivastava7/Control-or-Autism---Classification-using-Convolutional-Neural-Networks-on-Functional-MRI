{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ROI = \"cc400\"\n",
    "p_fold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list = [p_ROI]\n",
    "print(\"*****List of patameters****\")\n",
    "print(\"ROI atlas: \",p_ROI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from functools import reduce\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pyprind\n",
    "import sys\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "from sklearn import tree\n",
    "import functools\n",
    "import numpy.ma as ma # for masked arrays\n",
    "import pyprind\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(filename):\n",
    "    f_split = filename.split('_')\n",
    "    if f_split[3] == 'rois':\n",
    "        key = '_'.join(f_split[0:3]) \n",
    "    else:\n",
    "        key = '_'.join(f_split[0:2])\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_main_path = './data/rois_cc400/' #cc400#path to time series data\n",
    "flist = os.listdir(data_main_path)\n",
    "# flist = glob.glob('./data/rois_cc400/*.1D')\n",
    "print(len(flist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(flist)):\n",
    "    flist[f] = get_key(flist[f])\n",
    "    \n",
    "\n",
    "df_labels = pd.read_csv('Phenotypic_V1_0b_preprocessed1.csv')#path \n",
    "\n",
    "df_labels.DX_GROUP = df_labels.DX_GROUP.map({1: 1, 2:0})\n",
    "print(len(df_labels))\n",
    "\n",
    "labels = {}\n",
    "for row in df_labels.iterrows():\n",
    "    file_id = row[1]['FILE_ID']\n",
    "    y_label = row[1]['DX_GROUP']\n",
    "    if file_id == 'no_filename':\n",
    "        continue\n",
    "    assert(file_id not in labels)\n",
    "    labels[file_id] = y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(filename):\n",
    "    assert (filename in labels)\n",
    "    return labels[filename] \n",
    "\n",
    "\n",
    "def get_corr_data(filename):\n",
    "    # print(filename)\n",
    "    for file in os.listdir(data_main_path):\n",
    "        if file.startswith(filename):\n",
    "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
    "            # print(df)\n",
    "            \n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        # print(df.T)\n",
    "        # print(np.corrcoef(df.T).shape)\n",
    "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
    "        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n",
    "        # print(mask)\n",
    "        m = ma.masked_where(mask == 1, mask)\n",
    "        # print(m)\n",
    "\n",
    "        return ma.masked_where(m, corr).compressed()\n",
    "\n",
    "def get_corr_matrix(filename):\n",
    "    for file in os.listdir(data_main_path):\n",
    "        if file.startswith(filename):\n",
    "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
    "        return corr\n",
    "\n",
    "def confusion(g_turth,predictions):\n",
    "    tn, fp, fn, tp = confusion_matrix(g_turth,predictions).ravel()\n",
    "    accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
    "    sensitivity = (tp)/(tp+fn)\n",
    "    specificty = (tn)/(tn+fp)\n",
    "    return accuracy,sensitivity,specificty\n",
    "\n",
    "def get_regs(samplesnames,regnum):\n",
    "    datas = []\n",
    "    for sn in samplesnames:\n",
    "        datas.append(all_corr[sn][0])\n",
    "    datas = np.array(datas)\n",
    "    avg=[]\n",
    "    for ie in range(datas.shape[1]):\n",
    "        avg.append(np.mean(datas[:,ie]))\n",
    "    avg=np.array(avg)\n",
    "    highs=avg.argsort()[-regnum:][::-1]\n",
    "    lows=avg.argsort()[:regnum][::-1]\n",
    "    regions=np.concatenate((highs,lows),axis=0)\n",
    "    return regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./correlations_matrix'+p_ROI+'.pkl'):\n",
    "    pbar=pyprind.ProgBar(len(flist))\n",
    "    all_corr_mat = {}\n",
    "    for f in flist:\n",
    "      \n",
    "        lab = get_label(f)\n",
    "        all_corr_mat[f] = (get_corr_matrix(f), lab)\n",
    "        pbar.update()\n",
    "\n",
    "    print('Corr-computations finished')\n",
    "\n",
    "    pickle.dump(all_corr_mat, open('./correlations_matrix'+p_ROI+'.pkl', 'wb'))\n",
    "    print('Saving to file finished')\n",
    "else: \n",
    "    all_corr_mat = pickle.load(open('./correlations_matrix'+p_ROI+'.pkl', 'rb'))\n",
    "    print('loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corr_mat['Pitt_0050003'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corr_mat['CMU_a_0050647'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = all_corr_mat['CMU_a_0050647'][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('connetivity.npy', all_corr_mat['CMU_a_0050647'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CC400Dataset(Dataset):\n",
    "    def __init__(self, pkl_filename=None, data=None, samples_list=None):\n",
    "        if pkl_filename is not None:           \n",
    "            print ('Loading ..!', end=' ')\n",
    "            self.data = pickle.load(open(pkl_filename, 'rb'))\n",
    "            \n",
    "        elif data is not None:\n",
    "            self.data = data.copy()\n",
    "            \n",
    "        else:\n",
    "            sys.stderr.write('Eigther PKL file or data is needed!')\n",
    "            return \n",
    "\n",
    "        #if verbose:\n",
    "        #    print ('Preprocess..!', end='  ')\n",
    "        \n",
    "        if samples_list is None:\n",
    "            self.flist = [f for f in self.data]\n",
    "        else:\n",
    "            self.flist = [f for f in samples_list]\n",
    "        self.labels = np.array([self.data[f][1] for f in self.flist])\n",
    "        \n",
    "        current_flist = np.array(self.flist.copy())\n",
    "        current_lab0_flist = current_flist[self.labels == 0]\n",
    "        current_lab1_flist = current_flist[self.labels == 1]\n",
    "        #if verbose:\n",
    "        #    print(' Num Positive : ', len(current_lab1_flist), end=' ')\n",
    "        #    print(' Num Negative : ', len(current_lab0_flist), end=' ')\n",
    "              \n",
    "        self.num_data = len(self.flist)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        fname = self.flist[index]\n",
    "        data = self.data[fname][0].copy() #get_corr_data(fname, mode=cal_mode) \n",
    "        \n",
    "#         ## only taking upper triangle.. rest to zero\n",
    "#         data = np.array(data)\n",
    "#         data = np.triu(data,1)\n",
    "        \n",
    "        data = torch.FloatTensor(data)\n",
    "        data = torch.unsqueeze(data,0)\n",
    "\n",
    "\n",
    "#       print(data.shape)\n",
    "\n",
    "        #print(s.shape)\n",
    "        label = (self.labels[index],)\n",
    "        # print(label)\n",
    "        \n",
    "        return data, torch.FloatTensor(label)            \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(pkl_filename=None, data=None, samples_list=None,\n",
    "               batch_size=32, \n",
    "               num_workers=1, mode='train'):\n",
    "    \n",
    "    \"\"\"Build and return data loader.\"\"\"\n",
    "    if mode == 'train':\n",
    "        shuffle = True\n",
    "    else:\n",
    "        shuffle = False\n",
    "\n",
    "    dataset = CC400Dataset(pkl_filename=pkl_filename, data=data, samples_list=samples_list)\n",
    "\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             num_workers=num_workers)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evovle Norm Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evolving normalisation activation layers  \n",
    "\n",
    "from torch.nn import init\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "def instance_std(x, eps=1e-5):\n",
    "    N,C,H,W = x.size()\n",
    "    x1 = x.reshape(N*C,-1)\n",
    "    var = x1.var(dim=-1, keepdim=True)+eps\n",
    "    return var.sqrt().reshape(N,C,1,1)\n",
    "\n",
    "def group_std(x, groups, eps = 1e-5):\n",
    "    N, C, H, W = x.size()\n",
    "    x1 = x.reshape(N,groups,-1)\n",
    "    var = (x1.var(dim=-1, keepdim = True)+eps).reshape(N,groups,-1)\n",
    "    return (x1 / var.sqrt()).reshape(N,C,H,W)\n",
    "\n",
    "\n",
    "class BatchNorm2dRelu(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(BatchNorm2dRelu,self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True))\n",
    "    def forward(self, x):\n",
    "        output = self.layer(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class EvoNorm2dB0(nn.Module):\n",
    "    def __init__(self,in_channels,nonlinear=True,momentum=0.9,eps = 1e-5):\n",
    "        super(EvoNorm2dB0, self).__init__()\n",
    "        self.nonlinear = nonlinear\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.gamma = Parameter(torch.Tensor(1,in_channels,1,1))\n",
    "        self.beta = Parameter(torch.Tensor(1,in_channels,1,1))\n",
    "        if nonlinear:\n",
    "            self.v = Parameter(torch.Tensor(1,in_channels,1,1))\n",
    "        self.register_buffer('running_var', torch.ones(1, in_channels, 1, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.ones_(self.gamma)\n",
    "        init.zeros_(self.beta)\n",
    "        if self.nonlinear:\n",
    "            init.ones_(self.v)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size()\n",
    "        if self.training:\n",
    "            x1 = x.permute(1, 0, 2, 3).reshape(C, -1)\n",
    "            var = x1.var(dim=1).reshape(1, C, 1, 1)\n",
    "            self.running_var.copy_(self.momentum * self.running_var + (1 - self.momentum) * var)\n",
    "        else:\n",
    "            var = self.running_var\n",
    "        if self.nonlinear:\n",
    "            den = torch.max((var+self.eps).sqrt(), self.v * x + instance_std(x))\n",
    "            return x / den * self.gamma + self.beta\n",
    "        else:\n",
    "            return x * self.gamma + self.beta\n",
    "\n",
    "\n",
    "class EvoNorm2dS0(nn.Module):\n",
    "    def __init__(self,in_channels,groups=8,nonlinear=True):\n",
    "        super(EvoNorm2dS0, self).__init__()\n",
    "        self.nonlinear = nonlinear\n",
    "        self.groups = groups\n",
    "        self.gamma = Parameter(torch.Tensor(1,in_channels,1,1))\n",
    "        self.beta = Parameter(torch.Tensor(1,in_channels,1,1))\n",
    "        if nonlinear:\n",
    "            self.v = Parameter(torch.Tensor(1,in_channels,1,1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.ones_(self.gamma)\n",
    "        init.zeros_(self.beta)\n",
    "        if self.nonlinear:\n",
    "            init.ones_(self.v)\n",
    "    def forward(self, x):\n",
    "        if self.nonlinear:\n",
    "            num = torch.sigmoid(self.v * x)  \n",
    "            std = group_std(x,self.groups)\n",
    "            return num * std * self.gamma + self.beta\n",
    "        else:\n",
    "            return x * self.gamma + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convnet,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, (1,392)) \n",
    "        self.conv2 = nn.Conv2d(32, 64, (392,1))\n",
    "        \n",
    "        self.normact1 = EvoNorm2dS0(32, groups = 32) \n",
    "        self.normact2 = EvoNorm2dS0(64, groups = 32)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self.fc1 = nn.Linear(64,32)\n",
    "        self.fc2 = nn.Linear(32,1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p= 0.4)  \n",
    "        self.dropout2d = nn.Dropout2d(p=0.4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "     \n",
    "        x = self.normact1(self.conv1(x))  \n",
    "        x = self.normact2(self.conv2(x))\n",
    "              \n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.dropout(self.tanh(self.fc1(x)))\n",
    "#         print(x.size())\n",
    "        \n",
    "        x = self.fc2(x)  \n",
    "\n",
    "        return F.sigmoid(x)\n",
    "\n",
    "m = convnet() \n",
    "m.to(device)\n",
    "\n",
    "summary(m, (1 ,392 , 392 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, train_loader ):\n",
    "    \n",
    "    model.train()\n",
    "    batch_loss, ep_loss = 0.0,0.0\n",
    "    num = 0\n",
    "    \n",
    "    for i,(batch_x,batch_y) in enumerate(train_loader): \n",
    "        \n",
    "        data, target = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(data)\n",
    "\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss +=  loss.item() * len(batch_x) \n",
    "        \n",
    "        num += (len(batch_x))\n",
    "        \n",
    "#     scheduler.step()\n",
    "    ep_loss = batch_loss / num \n",
    "    print('train_epoch_loss: ', ep_loss) \n",
    "    \n",
    "    return ep_loss\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    n_test, correct = 0,0\n",
    "\n",
    "    all_predss=[] \n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for i,(batch_x,batch_y) in enumerate(test_loader, 1):\n",
    "\n",
    "            data = batch_x.to(device)\n",
    "\n",
    "            out = model(data)\n",
    "\n",
    "            proba = out.detach().cpu().numpy()\n",
    "\n",
    "            preds = np.ones_like(proba, dtype=np.int32)\n",
    "            preds[proba < 0.5] = 0\n",
    "\n",
    "            all_predss.extend(preds)\n",
    "\n",
    "            y_arr = np.array(batch_y, dtype=np.int32)\n",
    "            \n",
    "            correct += np.sum(preds == y_arr)\n",
    "            n_test += len(batch_x)\n",
    "            \n",
    "            y_true.extend(y_arr.tolist())\n",
    "            y_pred.extend(proba.tolist())\n",
    "\n",
    "        acc,sens,spec = confusion(y_true,all_predss)        \n",
    "          \n",
    "\n",
    "    return  acc,sens,spec, y_true, all_predss, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #weight initialisation Xavier for tanh and sigmoid act func\n",
    "def init_weights(m):\n",
    "    if (type(m) == nn.Linear or type(m) == nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start =time.time()\n",
    "batch_size = 32  ## for yielding good results in evolve-norm-act \n",
    "\n",
    "num_epochs = 300\n",
    "p_fold = 10 \n",
    "\n",
    "\n",
    "crossval_res_kol=[]\n",
    "# y_arr = np.array([get_label(f) for f in flist])\n",
    "flist = np.array(sorted(flist))\n",
    "\n",
    "kk=0 \n",
    "acc_avg = []\n",
    "sens_avg = []\n",
    "spec_avg = []\n",
    "\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(flist)\n",
    "y_arr = np.array([get_label(f) for f in flist])\n",
    "\n",
    "avg_acc = []\n",
    "true_all, pred_all, pred_proba = [], [], []\n",
    "\n",
    "for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
    "    train_samples, test_samples = flist[train_index], flist[test_index]\n",
    "    \n",
    "#     print(len(train_samples))\n",
    "#     print(len(test_samples))\n",
    "\n",
    "\n",
    "    train_loader=get_loader(data=all_corr_mat, samples_list=train_samples, \n",
    "                            batch_size=batch_size, mode='train')\n",
    "\n",
    "    test_loader=get_loader(data=all_corr_mat, samples_list=test_samples, \n",
    "                           batch_size=batch_size, mode='test')\n",
    "\n",
    "    model = convnet()\n",
    "    model.apply(init_weights)\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.01)  # good.. 0.7980\n",
    "        \n",
    "#     optimizer = optim.SGD(model.parameters(), lr = 0.1, momentum=0.9) \n",
    "    # step _size -- Period of learning rate decay.\n",
    "#     scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "#     scheduler = optim.lr_scheduler.ExponentialLR(optimizer,  gamma=0.9)\n",
    "#     scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr = 1e-3, max_lr=1e-1, step_size_up = 20, mode = 'exp_range' )    \n",
    "\n",
    "    print('k_fold:', kk )\n",
    "    \n",
    "    best_acc, j = 0.0, 0\n",
    "    best_true,best_pred, best_proba = [],[],[]\n",
    "    \n",
    "#     try_t = []\n",
    "#     try_p = []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print('epoch: ', epoch)\n",
    "\n",
    "        train_losses = train(model, epoch, train_loader)\n",
    "        acc, sens, spec,y_true,y_pred, y_proba = test(model, test_loader)\n",
    "#         print(y_pred)\n",
    "        \n",
    "        j = j + 1  \n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            print(\"Model_updated\")\n",
    "            best_true = y_true\n",
    "            best_pred = y_pred\n",
    "            best_proba = y_proba\n",
    "            \n",
    "            file_name = 'best_ACC' + str(kk) + '.pth' \n",
    "            torch.save(model,file_name)\n",
    "            j = 0\n",
    "            \n",
    "        else:  ## early stopping\n",
    "            print('patience', j)\n",
    "#         if j == 100:  # early stopping with patience 100  \n",
    "#             print('test_best_acc:', best_acc)\n",
    "#             break\n",
    "    \n",
    "        print('test_acc:', acc)\n",
    "        print('test_best_acc:', best_acc)    \n",
    "    \n",
    "#     break  \n",
    "    print('BEST_ACC: ' + str(best_acc) + \"&&\" + \"k-fold\" + str(kk))\n",
    "    avg_acc.append(best_acc)\n",
    "    true_all.append(best_true)\n",
    "    pred_all.append(best_pred)\n",
    "    pred_proba.append(best_proba)\n",
    "    \n",
    "#     acc_avg.append(acc)\n",
    "#     sens_avg.append(sens)\n",
    "#     spec_avg.append(spec)\n",
    "    \n",
    "    \n",
    "finish= time.time()\n",
    "\n",
    "print('Avg_test_acc: ', np.mean(avg_acc))\n",
    "# print('Avg_test_sens: ', np.mean(sens_avg))\n",
    "# print('Avg_test_spec: ', np.mean(spec_avg))\n",
    "print(finish-start)\n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_proba[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('true.npy', true_all )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pred.npy', pred_all )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pred_proba.npy', pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.load('true.npy', allow_pickle =  True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.load('pred.npy', allow_pickle =  True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = np.load('pred_proba.npy', allow_pickle =  True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
