{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 14 03:46:50 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        On   | 00000000:00:05.0 Off |                  N/A |\n",
      "| 26%   28C    P8     7W / 180W |      1MiB / 16278MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "WARNING: infoROM is corrupted at gpu 0000:00:05.0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phenotypic_V1_0b_preprocessed1.csv  correlations_matrixcc400.pkl\n",
      "Untitled.html\t\t\t    data\n",
      "asd_classify.html\t\t    datasets\n",
      "asd_classify.ipynb\t\t    download_abide.py\n",
      "asd_classify_conv_mod.html\t    storage\n",
      "asd_classify_conv_mod.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python download_abide.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ROI = \"cc400\"\n",
    "p_fold = 10\n",
    "#p_center = \"Stanford\"\n",
    "#p_mode = \"whole\"\n",
    "#p_augmentation = True\n",
    "#p_Method = \"ASD-DiagNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****List of patameters****\n",
      "ROI atlas:  cc400\n"
     ]
    }
   ],
   "source": [
    "parameter_list = [p_ROI]\n",
    "print(\"*****List of patameters****\")\n",
    "print(\"ROI atlas: \",p_ROI)\n",
    "# print(\"per Center or whole: \",p_mode)\n",
    "# if p_mode == 'percenter':\n",
    "#     print(\"Center's name: \",p_center)\n",
    "# print(\"Method's name: \",p_Method)\n",
    "# if p_Method == \"ASD-DiagNet\":\n",
    "#     print(\"Augmentation: \",p_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyprind in /usr/local/lib/python3.6/dist-packages (2.11.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyprind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from functools import reduce\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pyprind\n",
    "import sys\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "from sklearn import tree\n",
    "import functools\n",
    "import numpy.ma as ma # for masked arrays\n",
    "import pyprind\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(filename):\n",
    "    f_split = filename.split('_')\n",
    "    if f_split[3] == 'rois':\n",
    "        key = '_'.join(f_split[0:3]) \n",
    "    else:\n",
    "        key = '_'.join(f_split[0:2])\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1035\n"
     ]
    }
   ],
   "source": [
    "data_main_path = './data/rois_cc400/' #cc400#path to time series data\n",
    "flist = os.listdir(data_main_path)\n",
    "# flist = glob.glob('./data/rois_cc400/*.1D')\n",
    "print(len(flist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112\n"
     ]
    }
   ],
   "source": [
    "for f in range(len(flist)):\n",
    "    flist[f] = get_key(flist[f])\n",
    "    \n",
    "\n",
    "df_labels = pd.read_csv('Phenotypic_V1_0b_preprocessed1.csv')#path \n",
    "\n",
    "df_labels.DX_GROUP = df_labels.DX_GROUP.map({1: 1, 2:0})\n",
    "print(len(df_labels))\n",
    "\n",
    "labels = {}\n",
    "for row in df_labels.iterrows():\n",
    "    file_id = row[1]['FILE_ID']\n",
    "    y_label = row[1]['DX_GROUP']\n",
    "    if file_id == 'no_filename':\n",
    "        continue\n",
    "    assert(file_id not in labels)\n",
    "    labels[file_id] = y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(filename):\n",
    "    assert (filename in labels)\n",
    "    return labels[filename] \n",
    "\n",
    "\n",
    "def get_corr_data(filename):\n",
    "    # print(filename)\n",
    "    for file in os.listdir(data_main_path):\n",
    "        if file.startswith(filename):\n",
    "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
    "            # print(df)\n",
    "            \n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        # print(df.T)\n",
    "        # print(np.corrcoef(df.T).shape)\n",
    "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
    "        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n",
    "        # print(mask)\n",
    "        m = ma.masked_where(mask == 1, mask)\n",
    "        # print(m)\n",
    "\n",
    "        return ma.masked_where(m, corr).compressed()\n",
    "\n",
    "def get_corr_matrix(filename):\n",
    "    for file in os.listdir(data_main_path):\n",
    "        if file.startswith(filename):\n",
    "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
    "        return corr\n",
    "\n",
    "def confusion(g_turth,predictions):\n",
    "    tn, fp, fn, tp = confusion_matrix(g_turth,predictions).ravel()\n",
    "    accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
    "    sensitivity = (tp)/(tp+fn)\n",
    "    specificty = (tn)/(tn+fp)\n",
    "    return accuracy,sensitivity,specificty\n",
    "\n",
    "def get_regs(samplesnames,regnum):\n",
    "    datas = []\n",
    "    for sn in samplesnames:\n",
    "        datas.append(all_corr[sn][0])\n",
    "    datas = np.array(datas)\n",
    "    avg=[]\n",
    "    for ie in range(datas.shape[1]):\n",
    "        avg.append(np.mean(datas[:,ie]))\n",
    "    avg=np.array(avg)\n",
    "    highs=avg.argsort()[-regnum:][::-1]\n",
    "    lows=avg.argsort()[:regnum][::-1]\n",
    "    regions=np.concatenate((highs,lows),axis=0)\n",
    "    return regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./correlations_matrix'+p_ROI+'.pkl'):\n",
    "    pbar=pyprind.ProgBar(len(flist))\n",
    "    all_corr_mat = {}\n",
    "    for f in flist:\n",
    "      \n",
    "        lab = get_label(f)\n",
    "        all_corr_mat[f] = (get_corr_matrix(f), lab)\n",
    "        pbar.update()\n",
    "\n",
    "    print('Corr-computations finished')\n",
    "\n",
    "    pickle.dump(all_corr_mat, open('./correlations_matrix'+p_ROI+'.pkl', 'wb'))\n",
    "    print('Saving to file finished')\n",
    "else: \n",
    "    all_corr_mat = pickle.load(open('./correlations_matrix'+p_ROI+'.pkl', 'rb'))\n",
    "    print('loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 392)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_corr_mat['Pitt_0050003'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(eig_data[flist[0]]['eigvals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CC400Dataset(Dataset):\n",
    "    def __init__(self, pkl_filename=None, data=None, samples_list=None):\n",
    "        if pkl_filename is not None:           \n",
    "            print ('Loading ..!', end=' ')\n",
    "            self.data = pickle.load(open(pkl_filename, 'rb'))\n",
    "            \n",
    "        elif data is not None:\n",
    "            self.data = data.copy()\n",
    "            \n",
    "        else:\n",
    "            sys.stderr.write('Eigther PKL file or data is needed!')\n",
    "            return \n",
    "\n",
    "        #if verbose:\n",
    "        #    print ('Preprocess..!', end='  ')\n",
    "        \n",
    "        if samples_list is None:\n",
    "            self.flist = [f for f in self.data]\n",
    "        else:\n",
    "            self.flist = [f for f in samples_list]\n",
    "        self.labels = np.array([self.data[f][1] for f in self.flist])\n",
    "        \n",
    "        current_flist = np.array(self.flist.copy())\n",
    "        current_lab0_flist = current_flist[self.labels == 0]\n",
    "        current_lab1_flist = current_flist[self.labels == 1]\n",
    "        #if verbose:\n",
    "        #    print(' Num Positive : ', len(current_lab1_flist), end=' ')\n",
    "        #    print(' Num Negative : ', len(current_lab0_flist), end=' ')\n",
    "              \n",
    "        self.num_data = len(self.flist)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        fname = self.flist[index]\n",
    "        data = self.data[fname][0].copy() #get_corr_data(fname, mode=cal_mode) \n",
    "        \n",
    "#         ## only taking upper triangle.. rest to zero\n",
    "#         data = np.array(data)\n",
    "#         data = np.triu(data,1)\n",
    "        \n",
    "        data = torch.FloatTensor(data)\n",
    "        data = torch.unsqueeze(data,0)\n",
    "\n",
    "\n",
    "#       print(data.shape)\n",
    "\n",
    "        #print(s.shape)\n",
    "        label = (self.labels[index],)\n",
    "        # print(label)\n",
    "        \n",
    "        return data, torch.FloatTensor(label)            \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(pkl_filename=None, data=None, samples_list=None,\n",
    "               batch_size=32, \n",
    "               num_workers=1, mode='train'):\n",
    "    \n",
    "    \"\"\"Build and return data loader.\"\"\"\n",
    "    if mode == 'train':\n",
    "        shuffle = True\n",
    "    else:\n",
    "        shuffle = False\n",
    "\n",
    "    dataset = CC400Dataset(pkl_filename=pkl_filename, data=data, samples_list=samples_list)\n",
    "\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             num_workers=num_workers)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evovle Norm Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evolving normalisation activation layers  \n",
    "\n",
    "from torch.nn import init\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "def instance_std(x, eps=1e-5):\n",
    "    N,C,H,W = x.size()\n",
    "    x1 = x.reshape(N*C,-1)\n",
    "    var = x1.var(dim=-1, keepdim=True)+eps\n",
    "    return var.sqrt().reshape(N,C,1,1)\n",
    "\n",
    "def group_std(x, groups, eps = 1e-5):\n",
    "    N, C, H, W = x.size()\n",
    "    x1 = x.reshape(N,groups,-1)\n",
    "    var = (x1.var(dim=-1, keepdim = True)+eps).reshape(N,groups,-1)\n",
    "    return (x1 / var.sqrt()).reshape(N,C,H,W)\n",
    "\n",
    "\n",
    "class BatchNorm2dRelu(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(BatchNorm2dRelu,self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True))\n",
    "    def forward(self, x):\n",
    "        output = self.layer(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class EvoNorm2dB0(nn.Module):\n",
    "    def __init__(self,in_channels,nonlinear=True,momentum=0.9,eps = 1e-5):\n",
    "        super(EvoNorm2dB0, self).__init__()\n",
    "        self.nonlinear = nonlinear\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.gamma = Parameter(torch.Tensor(1,in_channels,1,1))\n",
    "        self.beta = Parameter(torch.Tensor(1,in_channels,1,1))\n",
    "        if nonlinear:\n",
    "            self.v = Parameter(torch.Tensor(1,in_channels,1,1))\n",
    "        self.register_buffer('running_var', torch.ones(1, in_channels, 1, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.ones_(self.gamma)\n",
    "        init.zeros_(self.beta)\n",
    "        if self.nonlinear:\n",
    "            init.ones_(self.v)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size()\n",
    "        if self.training:\n",
    "            x1 = x.permute(1, 0, 2, 3).reshape(C, -1)\n",
    "            var = x1.var(dim=1).reshape(1, C, 1, 1)\n",
    "            self.running_var.copy_(self.momentum * self.running_var + (1 - self.momentum) * var)\n",
    "        else:\n",
    "            var = self.running_var\n",
    "        if self.nonlinear:\n",
    "            den = torch.max((var+self.eps).sqrt(), self.v * x + instance_std(x))\n",
    "            return x / den * self.gamma + self.beta\n",
    "        else:\n",
    "            return x * self.gamma + self.beta\n",
    "\n",
    "\n",
    "class EvoNorm2dS0(nn.Module):\n",
    "    def __init__(self,in_channels,groups=8,nonlinear=True):\n",
    "        super(EvoNorm2dS0, self).__init__()\n",
    "        self.nonlinear = nonlinear\n",
    "        self.groups = groups\n",
    "        self.gamma = Parameter(torch.Tensor(1,in_channels,1,1))\n",
    "        self.beta = Parameter(torch.Tensor(1,in_channels,1,1))\n",
    "        if nonlinear:\n",
    "            self.v = Parameter(torch.Tensor(1,in_channels,1,1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.ones_(self.gamma)\n",
    "        init.zeros_(self.beta)\n",
    "        if self.nonlinear:\n",
    "            init.ones_(self.v)\n",
    "    def forward(self, x):\n",
    "        if self.nonlinear:\n",
    "            num = torch.sigmoid(self.v * x)\n",
    "            std = group_std(x,self.groups)\n",
    "            return num * std * self.gamma + self.beta\n",
    "        else:\n",
    "            return x * self.gamma + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 392, 1]          12,576\n",
      "       EvoNorm2dS0-2           [-1, 32, 392, 1]               0\n",
      "            Conv2d-3             [-1, 64, 1, 1]         802,880\n",
      "       EvoNorm2dS0-4             [-1, 64, 1, 1]               0\n",
      "            Conv2d-5           [-1, 32, 1, 392]          12,576\n",
      "       EvoNorm2dS0-6           [-1, 32, 1, 392]               0\n",
      "            Conv2d-7             [-1, 64, 1, 1]         802,880\n",
      "       EvoNorm2dS0-8             [-1, 64, 1, 1]               0\n",
      "           Dropout-9                  [-1, 128]               0\n",
      "           Linear-10                    [-1, 1]             129\n",
      "================================================================\n",
      "Total params: 1,631,041\n",
      "Trainable params: 1,631,041\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 0.39\n",
      "Params size (MB): 6.22\n",
      "Estimated Total Size (MB): 7.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class convnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convnet,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, (1,392))  ## with 32 channels 0.788 acc  ... exp \n",
    "        self.conv2 = nn.Conv2d(32, 64, (392,1))\n",
    "        self.conv3 = nn.Conv2d(1,32, (392,1))\n",
    "        self.conv4 = nn.Conv2d(32,64,(1,392) )\n",
    "        \n",
    "        self.normact1 = EvoNorm2dS0(32)\n",
    "        self.normact2 = EvoNorm2dS0(64) \n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(1, 64, (392,1))   ## normal cnn ## not using this one\n",
    "#         self.conv2 = nn.Conv2d(64, 128, (1,392)) \n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(1, 64, (1,392))   ## normal cnn ## used this one (1x392)\n",
    "#         self.conv2 = nn.Conv2d(64, 128, (392,1))  ##  Too.. Big .. :\\\n",
    "        \n",
    "\n",
    "#         self.normact1 = EvoNorm2dS0(64)   ## used this one..\n",
    "#         self.normact2 = EvoNorm2dS0(128)\n",
    "\n",
    "\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(128,1)\n",
    "#         self.fc = nn.Linear(128,1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p= 0.6)   ## in 0.6 taken \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        x1 = self.normact1(self.conv1(x))    ## exp \n",
    "        x2 = self.normact2(self.conv2(x1))\n",
    "\n",
    "        x3 = self.normact1(self.conv3(x))\n",
    "        x4 = self.normact2(self.conv4(x3))\n",
    "       \n",
    "    \n",
    "        x = torch.cat((x2,x4),1)\n",
    "        \n",
    "#         x = self.dropout(self.normact1(self.conv1(x)))  ## 0.817 acc ... give a check to it \n",
    "#         x = self.normact2(self.conv2(x)) \n",
    "        \n",
    "#         x = self.normact1(self.conv1(x)) \n",
    "#         x = self.normact2(self.conv2(x))\n",
    "\n",
    "#         x = self.bn1(F.relu(self.conv1(x)))\n",
    "#         x = self.bn2(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc(self.dropout(x))  \n",
    "\n",
    "        return F.sigmoid(x)\n",
    "\n",
    "m = convnet() \n",
    "m.to(device)\n",
    "\n",
    "summary(m, (1 ,392 , 392 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, train_loader ):\n",
    "    \n",
    "    model.train()\n",
    "    batch_loss, ep_loss = 0.0,0.0\n",
    "    num = 0\n",
    "    for i,(batch_x,batch_y) in enumerate(train_loader): \n",
    "        \n",
    "        data, target = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(data)\n",
    "\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss +=  loss.item() * len(batch_x) \n",
    "        \n",
    "        num += (len(batch_x))\n",
    "        \n",
    "    ep_loss = batch_loss / num \n",
    "    print('train_epoch_loss: ', ep_loss) \n",
    "\n",
    "    return ep_loss\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    n_test, correct = 0,0\n",
    "\n",
    "    all_predss=[] \n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for i,(batch_x,batch_y) in enumerate(test_loader, 1):\n",
    "\n",
    "            data = batch_x.to(device)\n",
    "\n",
    "            out = model(data)\n",
    "\n",
    "            proba = out.detach().cpu().numpy()\n",
    "\n",
    "            preds = np.ones_like(proba, dtype=np.int32)\n",
    "            preds[proba < 0.5] = 0\n",
    "\n",
    "            all_predss.extend(preds)\n",
    "\n",
    "            y_arr = np.array(batch_y, dtype=np.int32)\n",
    "            \n",
    "            correct += np.sum(preds == y_arr)\n",
    "            n_test += len(batch_x)\n",
    "            \n",
    "            y_true.extend(y_arr.tolist())\n",
    "            y_pred.extend(proba.tolist())\n",
    "        \n",
    "#         loss_avg = loss_total / n_test  \n",
    "        acc,sens,spec = confusion(y_true,all_predss)        \n",
    "        \n",
    "#         print('test_loss_avg:', loss_avg)\n",
    "#         print('test_acc:', acc )\n",
    "#         print('test_sens:', sens )\n",
    "#         print('test_spef:', spec )\n",
    "        \n",
    "\n",
    "    return  acc,sens,spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_corr[flist[0]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_epoch_loss:  0.6976231358361168\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6057692307692307\n",
      "epoch:  2\n",
      "train_epoch_loss:  0.6265081209475705\n",
      "patience 1\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6057692307692307\n",
      "epoch:  3\n",
      "train_epoch_loss:  0.5409867251341889\n",
      "patience 2\n",
      "test_acc: 0.5480769230769231\n",
      "test_best_acc: 0.6057692307692307\n",
      "epoch:  4\n",
      "train_epoch_loss:  0.43157814307064296\n",
      "patience 3\n",
      "test_acc: 0.5480769230769231\n",
      "test_best_acc: 0.6057692307692307\n",
      "epoch:  5\n",
      "train_epoch_loss:  0.3490129593363638\n",
      "patience 4\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6057692307692307\n",
      "epoch:  6\n",
      "train_epoch_loss:  0.24324563744244848\n",
      "test_acc: 0.625\n",
      "test_best_acc: 0.625\n",
      "epoch:  7\n",
      "train_epoch_loss:  0.22167643133218767\n",
      "patience 1\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.625\n",
      "epoch:  8\n",
      "train_epoch_loss:  0.08943496408415148\n",
      "patience 2\n",
      "test_acc: 0.5480769230769231\n",
      "test_best_acc: 0.625\n",
      "epoch:  9\n",
      "train_epoch_loss:  0.06791594856799801\n",
      "patience 3\n",
      "test_acc: 0.625\n",
      "test_best_acc: 0.625\n",
      "epoch:  10\n",
      "train_epoch_loss:  0.07313103292429486\n",
      "patience 4\n",
      "test_acc: 0.625\n",
      "test_best_acc: 0.625\n",
      "epoch:  11\n",
      "train_epoch_loss:  0.03882717600774528\n",
      "patience 5\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.625\n",
      "epoch:  12\n",
      "train_epoch_loss:  0.028071663737553147\n",
      "patience 6\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.625\n",
      "epoch:  13\n",
      "train_epoch_loss:  0.02559917394139841\n",
      "patience 7\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.625\n",
      "epoch:  14\n",
      "train_epoch_loss:  0.01944409154563598\n",
      "patience 8\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.625\n",
      "epoch:  15\n",
      "train_epoch_loss:  0.006024744566444334\n",
      "patience 9\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.625\n",
      "epoch:  16\n",
      "train_epoch_loss:  0.00584687042539233\n",
      "patience 10\n",
      "test_acc: 0.5480769230769231\n",
      "test_best_acc: 0.625\n",
      "epoch:  17\n",
      "train_epoch_loss:  0.006264749374248472\n",
      "patience 11\n",
      "test_acc: 0.625\n",
      "test_best_acc: 0.625\n",
      "epoch:  18\n",
      "train_epoch_loss:  0.01198039920739103\n",
      "patience 12\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.625\n",
      "epoch:  19\n",
      "train_epoch_loss:  0.009123399216532543\n",
      "patience 13\n",
      "test_acc: 0.6153846153846154\n",
      "test_best_acc: 0.625\n",
      "epoch:  20\n",
      "train_epoch_loss:  0.010424123083868501\n",
      "patience 14\n",
      "test_acc: 0.625\n",
      "test_best_acc: 0.625\n",
      "epoch:  21\n",
      "train_epoch_loss:  0.004269264171074548\n",
      "patience 15\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.625\n",
      "epoch:  22\n",
      "train_epoch_loss:  0.003638987287875364\n",
      "test_acc: 0.6346153846153846\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  23\n",
      "train_epoch_loss:  0.021161260711535346\n",
      "patience 1\n",
      "test_acc: 0.5576923076923077\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  24\n",
      "train_epoch_loss:  0.0662012294026854\n",
      "patience 2\n",
      "test_acc: 0.6153846153846154\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  25\n",
      "train_epoch_loss:  0.027685998485820762\n",
      "patience 3\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  26\n",
      "train_epoch_loss:  0.008639803974039925\n",
      "patience 4\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  27\n",
      "train_epoch_loss:  0.08994311137499025\n",
      "patience 5\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  28\n",
      "train_epoch_loss:  0.03679698682365145\n",
      "patience 6\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  29\n",
      "train_epoch_loss:  0.009579470844269869\n",
      "patience 7\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  30\n",
      "train_epoch_loss:  0.012227624300647243\n",
      "patience 8\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  31\n",
      "train_epoch_loss:  0.011559884197820901\n",
      "patience 9\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  32\n",
      "train_epoch_loss:  0.002919264685309559\n",
      "patience 10\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  33\n",
      "train_epoch_loss:  0.0017520398068217766\n",
      "patience 11\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  34\n",
      "train_epoch_loss:  0.0035487184804312176\n",
      "patience 12\n",
      "test_acc: 0.6153846153846154\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  35\n",
      "train_epoch_loss:  0.004823959593598107\n",
      "patience 13\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  36\n",
      "train_epoch_loss:  0.0046076462619237665\n",
      "patience 14\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  37\n",
      "train_epoch_loss:  0.002128010540596444\n",
      "patience 15\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  38\n",
      "train_epoch_loss:  0.0020748701581289287\n",
      "patience 16\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  39\n",
      "train_epoch_loss:  0.0005574139438109511\n",
      "patience 17\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  40\n",
      "train_epoch_loss:  0.0009508511960381646\n",
      "patience 18\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  41\n",
      "train_epoch_loss:  0.0005333385194085211\n",
      "patience 19\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  42\n",
      "train_epoch_loss:  0.0005523777228858118\n",
      "patience 20\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  43\n",
      "train_epoch_loss:  0.003295123190629563\n",
      "patience 21\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  44\n",
      "train_epoch_loss:  0.001521546716639297\n",
      "patience 22\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  45\n",
      "train_epoch_loss:  0.0004731796895869131\n",
      "patience 23\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  46\n",
      "train_epoch_loss:  0.0003752453022999849\n",
      "patience 24\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  47\n",
      "train_epoch_loss:  0.0001224343179530926\n",
      "patience 25\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  48\n",
      "train_epoch_loss:  0.0003805812025321267\n",
      "patience 26\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  49\n",
      "train_epoch_loss:  0.00016467395313541784\n",
      "patience 27\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  50\n",
      "train_epoch_loss:  0.00024543235606579324\n",
      "patience 28\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  51\n",
      "train_epoch_loss:  0.0002627822483413145\n",
      "patience 29\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  52\n",
      "train_epoch_loss:  0.0005574568245305063\n",
      "patience 30\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  53\n",
      "train_epoch_loss:  0.00045863576681432806\n",
      "patience 31\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  54\n",
      "train_epoch_loss:  0.0006716074504596975\n",
      "patience 32\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  55\n",
      "train_epoch_loss:  0.00032214222189115143\n",
      "patience 33\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  56\n",
      "train_epoch_loss:  0.0003583853660696274\n",
      "patience 34\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  57\n",
      "train_epoch_loss:  0.0008148746212362187\n",
      "patience 35\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  58\n",
      "train_epoch_loss:  0.0025538842560284274\n",
      "patience 36\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  59\n",
      "train_epoch_loss:  0.03746481996732859\n",
      "patience 37\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  60\n",
      "train_epoch_loss:  0.020022582770930452\n",
      "patience 38\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  61\n",
      "train_epoch_loss:  0.07421669120553713\n",
      "patience 39\n",
      "test_acc: 0.5192307692307693\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  62\n",
      "train_epoch_loss:  0.013834809198244044\n",
      "patience 40\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  63\n",
      "train_epoch_loss:  0.013567794924569401\n",
      "patience 41\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  64\n",
      "train_epoch_loss:  0.012201875319432318\n",
      "patience 42\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  65\n",
      "train_epoch_loss:  0.011035614009750517\n",
      "patience 43\n",
      "test_acc: 0.5480769230769231\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  66\n",
      "train_epoch_loss:  0.09094237683442942\n",
      "patience 44\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  67\n",
      "train_epoch_loss:  0.03265945560165814\n",
      "patience 45\n",
      "test_acc: 0.5096153846153846\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  68\n",
      "train_epoch_loss:  0.027607103001590663\n",
      "patience 46\n",
      "test_acc: 0.625\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  69\n",
      "train_epoch_loss:  0.01732309610750071\n",
      "patience 47\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  70\n",
      "train_epoch_loss:  0.009285354890921606\n",
      "patience 48\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  71\n",
      "train_epoch_loss:  0.0022411881795975307\n",
      "patience 49\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  72\n",
      "train_epoch_loss:  0.0013170259556794092\n",
      "patience 50\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  73\n",
      "train_epoch_loss:  0.0008819373294595847\n",
      "patience 51\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  74\n",
      "train_epoch_loss:  0.0004160076487438291\n",
      "patience 52\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  75\n",
      "train_epoch_loss:  0.00017792079324570082\n",
      "patience 53\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  76\n",
      "train_epoch_loss:  0.0005064649658076655\n",
      "patience 54\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  77\n",
      "train_epoch_loss:  0.0002443770318588809\n",
      "patience 55\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  78\n",
      "train_epoch_loss:  0.00024611498173248726\n",
      "patience 56\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  79\n",
      "train_epoch_loss:  9.272812414143293e-05\n",
      "patience 57\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  80\n",
      "train_epoch_loss:  0.00021304128387180093\n",
      "patience 58\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  81\n",
      "train_epoch_loss:  0.00011351775128155334\n",
      "patience 59\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  82\n",
      "train_epoch_loss:  0.0019390001076291876\n",
      "patience 60\n",
      "test_acc: 0.5576923076923077\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  83\n",
      "train_epoch_loss:  0.005182413827815712\n",
      "patience 61\n",
      "test_acc: 0.5576923076923077\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  84\n",
      "train_epoch_loss:  0.0016846518439049937\n",
      "patience 62\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  85\n",
      "train_epoch_loss:  0.0007863811698165011\n",
      "patience 63\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  86\n",
      "train_epoch_loss:  0.00084082505172451\n",
      "patience 64\n",
      "test_acc: 0.5576923076923077\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  87\n",
      "train_epoch_loss:  0.00030806290892595897\n",
      "patience 65\n",
      "test_acc: 0.5480769230769231\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  88\n",
      "train_epoch_loss:  0.0011707168972398786\n",
      "patience 66\n",
      "test_acc: 0.5576923076923077\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  89\n",
      "train_epoch_loss:  0.005196666449286388\n",
      "patience 67\n",
      "test_acc: 0.6153846153846154\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  90\n",
      "train_epoch_loss:  0.058398775734351475\n",
      "patience 68\n",
      "test_acc: 0.5576923076923077\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  91\n",
      "train_epoch_loss:  0.030689562051851794\n",
      "patience 69\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  92\n",
      "train_epoch_loss:  0.010954069363068782\n",
      "patience 70\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  93\n",
      "train_epoch_loss:  0.007454702530354342\n",
      "patience 71\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  94\n",
      "train_epoch_loss:  0.0026610652234976824\n",
      "patience 72\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  95\n",
      "train_epoch_loss:  0.0013635296819218298\n",
      "patience 73\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  96\n",
      "train_epoch_loss:  0.000457209203387836\n",
      "patience 74\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  97\n",
      "train_epoch_loss:  0.00048558461712985916\n",
      "patience 75\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  98\n",
      "train_epoch_loss:  0.0002580565218775208\n",
      "patience 76\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  99\n",
      "train_epoch_loss:  0.0002968637318092614\n",
      "patience 77\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  100\n",
      "train_epoch_loss:  0.00016950994287608125\n",
      "patience 78\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  101\n",
      "train_epoch_loss:  0.0001401090017576411\n",
      "patience 79\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  102\n",
      "train_epoch_loss:  0.00022241552470706575\n",
      "patience 80\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  103\n",
      "train_epoch_loss:  0.00027022818667377986\n",
      "patience 81\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  104\n",
      "train_epoch_loss:  0.0002069624726151217\n",
      "patience 82\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  105\n",
      "train_epoch_loss:  0.0010916076281035068\n",
      "patience 83\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  106\n",
      "train_epoch_loss:  0.0005001920258463265\n",
      "patience 84\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  107\n",
      "train_epoch_loss:  0.00034743174826141115\n",
      "patience 85\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  108\n",
      "train_epoch_loss:  0.00025142039448152695\n",
      "patience 86\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  109\n",
      "train_epoch_loss:  0.00016984163911847616\n",
      "patience 87\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  110\n",
      "train_epoch_loss:  0.0001639649034890726\n",
      "patience 88\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  111\n",
      "train_epoch_loss:  0.00018422410648713425\n",
      "patience 89\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  112\n",
      "train_epoch_loss:  4.775886049209174e-05\n",
      "patience 90\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  113\n",
      "train_epoch_loss:  0.00024684282223492456\n",
      "patience 91\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  114\n",
      "train_epoch_loss:  7.732511892483123e-05\n",
      "patience 92\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  115\n",
      "train_epoch_loss:  0.0015078416367497337\n",
      "patience 93\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  116\n",
      "train_epoch_loss:  0.00023584969982218932\n",
      "patience 94\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  117\n",
      "train_epoch_loss:  0.0006141729362421643\n",
      "patience 95\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  118\n",
      "train_epoch_loss:  0.0003920257809696014\n",
      "patience 96\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  119\n",
      "train_epoch_loss:  0.00046481184394989486\n",
      "patience 97\n",
      "test_acc: 0.5576923076923077\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  120\n",
      "train_epoch_loss:  0.000408313825535737\n",
      "patience 98\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  121\n",
      "train_epoch_loss:  9.925325040635779e-05\n",
      "patience 99\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  122\n",
      "train_epoch_loss:  5.147670013082776e-05\n",
      "patience 100\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  123\n",
      "train_epoch_loss:  0.00040922739627270607\n",
      "patience 101\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  124\n",
      "train_epoch_loss:  0.0002830745140735932\n",
      "patience 102\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  125\n",
      "train_epoch_loss:  9.371730541527265e-05\n",
      "patience 103\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  126\n",
      "train_epoch_loss:  0.00029180051556229256\n",
      "patience 104\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  127\n",
      "train_epoch_loss:  8.970070676265574e-05\n",
      "patience 105\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  128\n",
      "train_epoch_loss:  5.179017931093315e-05\n",
      "patience 106\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  129\n",
      "train_epoch_loss:  0.00014346024499070157\n",
      "patience 107\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  130\n",
      "train_epoch_loss:  0.00011940378181159984\n",
      "patience 108\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  131\n",
      "train_epoch_loss:  3.9863864288842177e-05\n",
      "patience 109\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  132\n",
      "train_epoch_loss:  6.319451870203666e-05\n",
      "patience 110\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  133\n",
      "train_epoch_loss:  0.001903283291807663\n",
      "patience 111\n",
      "test_acc: 0.5480769230769231\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  134\n",
      "train_epoch_loss:  0.011610947398103226\n",
      "patience 112\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  135\n",
      "train_epoch_loss:  0.009589411396831917\n",
      "patience 113\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  136\n",
      "train_epoch_loss:  0.004624011512311132\n",
      "patience 114\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  137\n",
      "train_epoch_loss:  0.0007031274140161231\n",
      "patience 115\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  138\n",
      "train_epoch_loss:  0.004823425065207863\n",
      "patience 116\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  139\n",
      "train_epoch_loss:  0.005458507237197666\n",
      "patience 117\n",
      "test_acc: 0.5480769230769231\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  140\n",
      "train_epoch_loss:  0.030452190968820726\n",
      "patience 118\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  141\n",
      "train_epoch_loss:  0.03573040353488397\n",
      "patience 119\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  142\n",
      "train_epoch_loss:  0.009772961124991465\n",
      "patience 120\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  143\n",
      "train_epoch_loss:  0.007019238465881695\n",
      "patience 121\n",
      "test_acc: 0.6153846153846154\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  144\n",
      "train_epoch_loss:  0.0105178402494199\n",
      "patience 122\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  145\n",
      "train_epoch_loss:  0.01609994409817559\n",
      "patience 123\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  146\n",
      "train_epoch_loss:  0.005027694503349581\n",
      "patience 124\n",
      "test_acc: 0.6153846153846154\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  147\n",
      "train_epoch_loss:  0.01630316956681203\n",
      "patience 125\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  148\n",
      "train_epoch_loss:  0.00329738791239729\n",
      "patience 126\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  149\n",
      "train_epoch_loss:  0.0010703589376332247\n",
      "patience 127\n",
      "test_acc: 0.6153846153846154\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  150\n",
      "train_epoch_loss:  0.0005961386361492937\n",
      "patience 128\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  151\n",
      "train_epoch_loss:  0.00011919603964120548\n",
      "patience 129\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  152\n",
      "train_epoch_loss:  9.500880982809904e-05\n",
      "patience 130\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  153\n",
      "train_epoch_loss:  0.00020379314800953252\n",
      "patience 131\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  154\n",
      "train_epoch_loss:  0.0004666322222119297\n",
      "patience 132\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  155\n",
      "train_epoch_loss:  0.00011383224767384552\n",
      "patience 133\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  156\n",
      "train_epoch_loss:  5.585422364952181e-05\n",
      "patience 134\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  157\n",
      "train_epoch_loss:  9.044564065043154e-05\n",
      "patience 135\n",
      "test_acc: 0.6153846153846154\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  158\n",
      "train_epoch_loss:  8.302752031115124e-05\n",
      "patience 136\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  159\n",
      "train_epoch_loss:  0.00018475078388819892\n",
      "patience 137\n",
      "test_acc: 0.6153846153846154\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  160\n",
      "train_epoch_loss:  8.311284346759782e-05\n",
      "patience 138\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  161\n",
      "train_epoch_loss:  0.00012898846149054163\n",
      "patience 139\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  162\n",
      "train_epoch_loss:  2.858416410182583e-05\n",
      "patience 140\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  163\n",
      "train_epoch_loss:  0.00017679398770347623\n",
      "patience 141\n",
      "test_acc: 0.625\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  164\n",
      "train_epoch_loss:  2.5943145450383673e-05\n",
      "patience 142\n",
      "test_acc: 0.625\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  165\n",
      "train_epoch_loss:  0.00016246526700969827\n",
      "patience 143\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  166\n",
      "train_epoch_loss:  2.5766531003492506e-05\n",
      "patience 144\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  167\n",
      "train_epoch_loss:  3.406482342648682e-05\n",
      "patience 145\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  168\n",
      "train_epoch_loss:  6.734347114523274e-05\n",
      "patience 146\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  169\n",
      "train_epoch_loss:  5.890346516547267e-05\n",
      "patience 147\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  170\n",
      "train_epoch_loss:  0.0003473494106813059\n",
      "patience 148\n",
      "test_acc: 0.6153846153846154\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  171\n",
      "train_epoch_loss:  2.2580599055270596e-05\n",
      "patience 149\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  172\n",
      "train_epoch_loss:  1.8191373321996918e-05\n",
      "patience 150\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  173\n",
      "train_epoch_loss:  3.160997269486539e-05\n",
      "patience 151\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  174\n",
      "train_epoch_loss:  2.2269012153981953e-05\n",
      "patience 152\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  175\n",
      "train_epoch_loss:  2.4468783170432027e-05\n",
      "patience 153\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  176\n",
      "train_epoch_loss:  2.678762907327522e-05\n",
      "patience 154\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  177\n",
      "train_epoch_loss:  1.7289069598606023e-05\n",
      "patience 155\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  178\n",
      "train_epoch_loss:  9.882864513909404e-06\n",
      "patience 156\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  179\n",
      "train_epoch_loss:  0.00016207493604196717\n",
      "patience 157\n",
      "test_acc: 0.6346153846153846\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  180\n",
      "train_epoch_loss:  3.7419614656229765e-05\n",
      "patience 158\n",
      "test_acc: 0.6346153846153846\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  181\n",
      "train_epoch_loss:  7.379359434489386e-05\n",
      "patience 159\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  182\n",
      "train_epoch_loss:  1.6677078538280693e-05\n",
      "patience 160\n",
      "test_acc: 0.5576923076923077\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  183\n",
      "train_epoch_loss:  1.868318787208196e-05\n",
      "patience 161\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  184\n",
      "train_epoch_loss:  1.019410860782396e-05\n",
      "patience 162\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  185\n",
      "train_epoch_loss:  2.346519454696941e-05\n",
      "patience 163\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  186\n",
      "train_epoch_loss:  4.1920400989219014e-05\n",
      "patience 164\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  187\n",
      "train_epoch_loss:  2.2184711096157353e-05\n",
      "patience 165\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  188\n",
      "train_epoch_loss:  2.948406262242576e-05\n",
      "patience 166\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  189\n",
      "train_epoch_loss:  5.254273860782132e-06\n",
      "patience 167\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  190\n",
      "train_epoch_loss:  4.2034563236721636e-06\n",
      "patience 168\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  191\n",
      "train_epoch_loss:  1.0947980902926396e-05\n",
      "patience 169\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  192\n",
      "train_epoch_loss:  1.1521780048887415e-05\n",
      "patience 170\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  193\n",
      "train_epoch_loss:  2.3430960280834045e-05\n",
      "patience 171\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  194\n",
      "train_epoch_loss:  8.677822798646516e-06\n",
      "patience 172\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  195\n",
      "train_epoch_loss:  1.2071131259700568e-05\n",
      "patience 173\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  196\n",
      "train_epoch_loss:  1.7047475570254212e-05\n",
      "patience 174\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  197\n",
      "train_epoch_loss:  3.365243530447246e-05\n",
      "patience 175\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  198\n",
      "train_epoch_loss:  7.677792901655066e-06\n",
      "patience 176\n",
      "test_acc: 0.5576923076923077\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  199\n",
      "train_epoch_loss:  1.671672836524887e-05\n",
      "patience 177\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  200\n",
      "train_epoch_loss:  1.9366315181718487e-05\n",
      "patience 178\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  201\n",
      "train_epoch_loss:  1.2540579984808856e-05\n",
      "patience 179\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  202\n",
      "train_epoch_loss:  6.177795140896653e-06\n",
      "patience 180\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  203\n",
      "train_epoch_loss:  5.143290285882842e-06\n",
      "patience 181\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  204\n",
      "train_epoch_loss:  4.569528147811356e-06\n",
      "patience 182\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  205\n",
      "train_epoch_loss:  6.425186427777521e-06\n",
      "patience 183\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  206\n",
      "train_epoch_loss:  2.6215872578933122e-05\n",
      "patience 184\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  207\n",
      "train_epoch_loss:  5.491225923860861e-06\n",
      "patience 185\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  208\n",
      "train_epoch_loss:  1.9964673493095238e-05\n",
      "patience 186\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  209\n",
      "train_epoch_loss:  1.3193921052624329e-05\n",
      "patience 187\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  210\n",
      "train_epoch_loss:  4.267239304042532e-05\n",
      "patience 188\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  211\n",
      "train_epoch_loss:  3.108513130307634e-06\n",
      "patience 189\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  212\n",
      "train_epoch_loss:  4.690496949390086e-05\n",
      "patience 190\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  213\n",
      "train_epoch_loss:  6.5250692909237316e-06\n",
      "patience 191\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  214\n",
      "train_epoch_loss:  4.9260272405500484e-06\n",
      "patience 192\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  215\n",
      "train_epoch_loss:  8.610834969176164e-06\n",
      "patience 193\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  216\n",
      "train_epoch_loss:  3.746978932159367e-06\n",
      "patience 194\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  217\n",
      "train_epoch_loss:  8.130399358124298e-06\n",
      "patience 195\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  218\n",
      "train_epoch_loss:  1.1331869994020883e-05\n",
      "patience 196\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  219\n",
      "train_epoch_loss:  7.70642797026327e-06\n",
      "patience 197\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  220\n",
      "train_epoch_loss:  8.461068750991719e-06\n",
      "patience 198\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  221\n",
      "train_epoch_loss:  2.3547341273080626e-05\n",
      "patience 199\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  222\n",
      "train_epoch_loss:  1.1006584406526336e-05\n",
      "patience 200\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  223\n",
      "train_epoch_loss:  1.760534997747452e-06\n",
      "patience 201\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  224\n",
      "train_epoch_loss:  5.840885155037826e-06\n",
      "patience 202\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  225\n",
      "train_epoch_loss:  2.7776107130343803e-06\n",
      "patience 203\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  226\n",
      "train_epoch_loss:  1.908934518528601e-06\n",
      "patience 204\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  227\n",
      "train_epoch_loss:  5.793748184851552e-06\n",
      "patience 205\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  228\n",
      "train_epoch_loss:  1.016049737398102e-05\n",
      "patience 206\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  229\n",
      "train_epoch_loss:  7.387204961939238e-06\n",
      "patience 207\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  230\n",
      "train_epoch_loss:  3.832426224375292e-06\n",
      "patience 208\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  231\n",
      "train_epoch_loss:  2.5500835994209947e-06\n",
      "patience 209\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  232\n",
      "train_epoch_loss:  4.536657579045364e-06\n",
      "patience 210\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6346153846153846\n",
      "epoch:  233\n",
      "train_epoch_loss:  5.93573816255429e-05\n",
      "test_acc: 0.6442307692307693\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  234\n",
      "train_epoch_loss:  1.2843530672939832e-05\n",
      "patience 1\n",
      "test_acc: 0.6442307692307693\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  235\n",
      "train_epoch_loss:  3.9271059421588896e-05\n",
      "patience 2\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  236\n",
      "train_epoch_loss:  5.650390555132115e-06\n",
      "patience 3\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  237\n",
      "train_epoch_loss:  6.343390479532538e-06\n",
      "patience 4\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  238\n",
      "train_epoch_loss:  5.502444638878577e-06\n",
      "patience 5\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  239\n",
      "train_epoch_loss:  1.0708246577941423e-05\n",
      "patience 6\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  240\n",
      "train_epoch_loss:  9.558913357896987e-06\n",
      "patience 7\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  241\n",
      "train_epoch_loss:  9.998525893874538e-06\n",
      "patience 8\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  242\n",
      "train_epoch_loss:  2.0972051890008158e-06\n",
      "patience 9\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  243\n",
      "train_epoch_loss:  1.8227459160842925e-06\n",
      "patience 10\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  244\n",
      "train_epoch_loss:  7.216145527410335e-06\n",
      "patience 11\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  245\n",
      "train_epoch_loss:  2.2373658270353833e-06\n",
      "patience 12\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  246\n",
      "train_epoch_loss:  2.0801297132296746e-06\n",
      "patience 13\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  247\n",
      "train_epoch_loss:  9.694299387139677e-06\n",
      "patience 14\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  248\n",
      "train_epoch_loss:  9.094624059524885e-06\n",
      "patience 15\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  249\n",
      "train_epoch_loss:  6.282229819250229e-06\n",
      "patience 16\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  250\n",
      "train_epoch_loss:  2.7119537524638738e-05\n",
      "patience 17\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  251\n",
      "train_epoch_loss:  2.4494250917971485e-06\n",
      "patience 18\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  252\n",
      "train_epoch_loss:  3.333639378295128e-06\n",
      "patience 19\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  253\n",
      "train_epoch_loss:  2.7695583028623806e-06\n",
      "patience 20\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  254\n",
      "train_epoch_loss:  1.096696458450334e-06\n",
      "patience 21\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  255\n",
      "train_epoch_loss:  1.5925323623774928e-06\n",
      "patience 22\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  256\n",
      "train_epoch_loss:  1.6457723836990024e-06\n",
      "patience 23\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  257\n",
      "train_epoch_loss:  1.2786956991792988e-05\n",
      "patience 24\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  258\n",
      "train_epoch_loss:  4.640001462078063e-06\n",
      "patience 25\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  259\n",
      "train_epoch_loss:  3.1349255658297854e-06\n",
      "patience 26\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  260\n",
      "train_epoch_loss:  1.9861368429416287e-06\n",
      "patience 27\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  261\n",
      "train_epoch_loss:  6.852537853481195e-07\n",
      "patience 28\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  262\n",
      "train_epoch_loss:  2.293143240854225e-06\n",
      "patience 29\n",
      "test_acc: 0.5961538461538461\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  263\n",
      "train_epoch_loss:  3.361902814628791e-06\n",
      "patience 30\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  264\n",
      "train_epoch_loss:  3.6308692026891982e-06\n",
      "patience 31\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  265\n",
      "train_epoch_loss:  3.991418948846974e-06\n",
      "patience 32\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  266\n",
      "train_epoch_loss:  3.3255682259748246e-06\n",
      "patience 33\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  267\n",
      "train_epoch_loss:  1.683696688711044e-06\n",
      "patience 34\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  268\n",
      "train_epoch_loss:  1.8439398163857835e-05\n",
      "patience 35\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  269\n",
      "train_epoch_loss:  1.254074565025179e-05\n",
      "patience 36\n",
      "test_acc: 0.6442307692307693\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  270\n",
      "train_epoch_loss:  3.368537065611378e-05\n",
      "patience 37\n",
      "test_acc: 0.5576923076923077\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  271\n",
      "train_epoch_loss:  6.051993738255362e-06\n",
      "patience 38\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  272\n",
      "train_epoch_loss:  6.460399992546716e-06\n",
      "patience 39\n",
      "test_acc: 0.5673076923076923\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  273\n",
      "train_epoch_loss:  6.464778780599954e-06\n",
      "patience 40\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  274\n",
      "train_epoch_loss:  5.841324037503462e-06\n",
      "patience 41\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  275\n",
      "train_epoch_loss:  4.858204925207243e-06\n",
      "patience 42\n",
      "test_acc: 0.6153846153846154\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  276\n",
      "train_epoch_loss:  7.501241180743936e-06\n",
      "patience 43\n",
      "test_acc: 0.6346153846153846\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  277\n",
      "train_epoch_loss:  6.552845048318988e-06\n",
      "patience 44\n",
      "test_acc: 0.625\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  278\n",
      "train_epoch_loss:  5.281994270080832e-06\n",
      "patience 45\n",
      "test_acc: 0.6153846153846154\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  279\n",
      "train_epoch_loss:  2.4108696510237192e-06\n",
      "patience 46\n",
      "test_acc: 0.6153846153846154\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  280\n",
      "train_epoch_loss:  8.484365579554429e-07\n",
      "patience 47\n",
      "test_acc: 0.6153846153846154\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  281\n",
      "train_epoch_loss:  5.087824631870657e-06\n",
      "patience 48\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  282\n",
      "train_epoch_loss:  7.108235447983732e-07\n",
      "patience 49\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  283\n",
      "train_epoch_loss:  4.32641456685359e-06\n",
      "patience 50\n",
      "test_acc: 0.6057692307692307\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  284\n",
      "train_epoch_loss:  6.089910733835671e-06\n",
      "patience 51\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  285\n",
      "train_epoch_loss:  2.8970502780072362e-06\n",
      "patience 52\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  286\n",
      "train_epoch_loss:  5.1279026367148495e-06\n",
      "patience 53\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  287\n",
      "train_epoch_loss:  5.556711653072499e-07\n",
      "patience 54\n",
      "test_acc: 0.5865384615384616\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  288\n",
      "train_epoch_loss:  4.170467897220752e-06\n",
      "patience 55\n",
      "test_acc: 0.5769230769230769\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  289\n",
      "train_epoch_loss:  1.532765244651339e-05\n",
      "patience 56\n",
      "test_acc: 0.6442307692307693\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  290\n",
      "train_epoch_loss:  1.4450115670940087e-06\n",
      "patience 57\n",
      "test_acc: 0.6442307692307693\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  291\n",
      "train_epoch_loss:  2.7032300459437546e-06\n",
      "patience 58\n",
      "test_acc: 0.6442307692307693\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  292\n",
      "train_epoch_loss:  2.4197971624541534e-06\n",
      "patience 59\n",
      "test_acc: 0.6442307692307693\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  293\n",
      "train_epoch_loss:  5.411307529949391e-07\n",
      "patience 60\n",
      "test_acc: 0.6442307692307693\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  294\n",
      "train_epoch_loss:  1.0164132419521165e-06\n",
      "patience 61\n",
      "test_acc: 0.6442307692307693\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  295\n",
      "train_epoch_loss:  1.0337490015554585e-06\n",
      "patience 62\n",
      "test_acc: 0.6442307692307693\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  296\n",
      "train_epoch_loss:  8.078652725775126e-07\n",
      "patience 63\n",
      "test_acc: 0.6442307692307693\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  297\n",
      "train_epoch_loss:  3.855751469719046e-06\n",
      "patience 64\n",
      "test_acc: 0.6346153846153846\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  298\n",
      "train_epoch_loss:  7.278541880551577e-07\n",
      "patience 65\n",
      "test_acc: 0.6346153846153846\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  299\n",
      "train_epoch_loss:  1.2283216818298519e-06\n",
      "patience 66\n",
      "test_acc: 0.625\n",
      "test_best_acc: 0.6442307692307693\n",
      "epoch:  300\n",
      "train_epoch_loss:  1.0454855143286767e-06\n",
      "patience 67\n",
      "test_acc: 0.625\n",
      "test_best_acc: 0.6442307692307693\n",
      "427.2979941368103\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "start =time.time()\n",
    "batch_size = 32  ## for yielding good results in evolve-norm-act \n",
    "\n",
    "num_epochs = 300  \n",
    "p_fold = 10 \n",
    "\n",
    "\n",
    "crossval_res_kol=[]\n",
    "# y_arr = np.array([get_label(f) for f in flist])\n",
    "flist = np.array(sorted(flist))\n",
    "\n",
    "kk=0 \n",
    "acc_avg = []\n",
    "sens_avg = []\n",
    "spec_avg = []\n",
    "#     for rp in range(10):\n",
    "\n",
    "#         print('whole_iterate:', rp)\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(flist)\n",
    "y_arr = np.array([get_label(f) for f in flist])\n",
    "\n",
    "for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
    "    train_samples, test_samples = flist[train_index], flist[test_index]\n",
    "    \n",
    "#     print(len(train_samples))\n",
    "#     print(len(test_samples))\n",
    "\n",
    "\n",
    "    train_loader=get_loader(data=all_corr_mat, samples_list=train_samples, \n",
    "                            batch_size=batch_size, mode='train')\n",
    "\n",
    "    test_loader=get_loader(data=all_corr_mat, samples_list=test_samples, \n",
    "                           batch_size=batch_size, mode='test')\n",
    "\n",
    "    model = convnet()\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss() \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "#     print('k_fold:', kk )\n",
    "    \n",
    "    best_acc, j = 0.0, 0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print('epoch: ', epoch)\n",
    "\n",
    "        train_losses = train(model, epoch, train_loader)\n",
    "        acc, sens, spec = test(model, test_loader)\n",
    "        \n",
    "        j = j + 1  \n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            j = 0\n",
    "            \n",
    "        else:  ## early stopping\n",
    "            print('patience', j)\n",
    "#         if j == 100:  # early stopping with patience 100  \n",
    "#             print('test_best_acc:', best_acc)\n",
    "#             break\n",
    "        print('test_acc:', acc)\n",
    "        print('test_best_acc:', best_acc)\n",
    "                   \n",
    "    break\n",
    "    \n",
    "#     acc_avg.append(acc)\n",
    "#     sens_avg.append(sens)\n",
    "#     spec_avg.append(spec)\n",
    "    \n",
    "    \n",
    "finish= time.time()\n",
    "\n",
    "# print('Avg_test_acc: ', np.mean(acc_avg))\n",
    "# print('Avg_test_sens: ', np.mean(sens_avg))\n",
    "# print('Avg_test_spec: ', np.mean(spec_avg))\n",
    "print(finish-start)\n",
    "print('Done !')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
